{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both clean_dataset.txt and noisy_dataset.txt into a list\n",
    "clean = np.loadtxt(\"co395-cbc-dt/wifi_db/clean_dataset.txt\", usecols= (0,1,2,3,4,5,6,7),unpack= False)\n",
    "noisy = np.loadtxt(\"co395-cbc-dt/wifi_db/noisy_dataset.txt\", usecols= (0,1,2,3,4,5,6,7),unpack= False)\n",
    "\n",
    "temp = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A structure for nodes of the decision tree\n",
    "class TreeNode:\n",
    "    def __init__(self, v, lc, rc):\n",
    "        self.nodeValue = v\n",
    "        self.lChild = lc\n",
    "        self.rChild = rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_learning(dataset, depth):\n",
    "    # Check the last column(labels) and if all of them are samely labeled, return this dataset as a leaf\n",
    "    if len(set([data[-1] for data in dataset])) == 1:\n",
    "        return (TreeNode((7,dataset[0][7]), None, None), depth)\n",
    "    else:\n",
    "        node = find_split(dataset)\n",
    "        temp = list(dataset)\n",
    "        # Sort the dataset w.r.t the column's value of every datum.\n",
    "        dataset = np.array(sorted(temp, key=lambda x:(x[node[0]], x[7]))) \n",
    "        (l_dataset, r_dataset) = split_set(dataset, node[1], node[0])\n",
    "        (l_branch, l_depth) = decision_tree_learning(l_dataset, depth + 1)\n",
    "        (r_branch, r_depth) = decision_tree_learning(r_dataset, depth + 1)\n",
    "        return (TreeNode(node, l_branch, r_branch), max(l_depth, r_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a treeNode with the splitting column and the splitting point stored\n",
    "def find_split(dataset): \n",
    "    column_length = len(dataset[0])\n",
    "    max_gain = 0\n",
    "    max_gain_split_value = 0\n",
    "    column_no = 0;\n",
    "    for column in range(column_length):\n",
    "         (max_gain_col, max_gain_split_point_col) = find_split_point_for_column(dataset, column)\n",
    "         if max_gain_col > max_gain:\n",
    "             max_gain = max_gain_col\n",
    "             max_gain_split_value = max_gain_split_point_col\n",
    "             column_no = column\n",
    "    return (column_no, max_gain_split_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a best gaining splitting point and its gain in tuples\n",
    "def find_split_point_for_column(dataset, column): \n",
    "    temp = list(dataset)\n",
    "    # Sort the dataset w.r.t the column's value of every datum.\n",
    "    dataset = np.array(sorted(temp, key=lambda x:(x[column], x[7]))) \n",
    "    biggest_gain = 0 # Store the biggest gain.\n",
    "    biggest_gain_splitting_point = 0\n",
    "    row_length = len(dataset)\n",
    "    for num in range(row_length):\n",
    "        # Find a split point.\n",
    "        if (dataset[num][7] != dataset[num+1][7]) and (dataset[num][column] != dataset[num+1][column]): \n",
    "            split_value = dataset[num+1][column]\n",
    "            (set_left, set_right) = split_set(dataset, split_value, column)\n",
    "            gain = calculate_gain(dataset, set_left, set_right)\n",
    "            if biggest_gain < gain:\n",
    "                biggest_gain = gain\n",
    "                biggest_gain_splitting_point = split_value\n",
    "\n",
    "    return (biggest_gain, biggest_gain_splitting_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set(data, split_value, column):\n",
    "    set_left = []\n",
    "    set_right = []\n",
    "    for datum in data:\n",
    "        if datum[column] < split_value:\n",
    "            set_left.append(datum)\n",
    "        else:\n",
    "            set_right.append(datum)\n",
    "    return (set_left, set_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the gain for the dataset and the given left set and right set.\n",
    "def calculate_gain(dataset, left, right): \n",
    "    left_size = len(left)\n",
    "    right_size = len(right)\n",
    "    remainder = left_size/len(dataset) * calculate_enthropy(left) + right_size/len(dataset) * calculate_enthropy(right)\n",
    "    return calculate_enthropy(dataset) - remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the enthropy for the dataset.\n",
    "def calculate_enthropy(dataset): \n",
    "    p = [0,0,0,0]\n",
    "    entropy = 0\n",
    "    for data in dataset:\n",
    "        p[np.int(data[7])-1] += 1\n",
    "    for prob in p:\n",
    "        entropy -= math.log2(prob/len(dataset)) * prob/len(dataset)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
